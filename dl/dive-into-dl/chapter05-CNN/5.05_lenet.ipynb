{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 卷积神经网络(LeNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNet网络结构](img/5.5_lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 LeNet模型\n",
    "\n",
    "1. **分为卷积层块和全链接层块**\n",
    "2. **卷积层基本单位: 卷积层(5x5,且sigmoid激活)+最大池化层(2x2)**\n",
    "    - 卷积层: 识别图像的空间模型,如线条和物体局部\n",
    "    - 最大池化: 降低卷积层对位置的敏感性\n",
    "3. **C1层(卷积层):6@28x28**\n",
    "    - 特诊图大小: 6个卷积核,得到feature map大小为(32-5+1)x(32-5+1)=28x28\n",
    "    - 参数个数: 因为权值共享,所以参数个数 (5x5+1)x6 = 156 (5x5为卷积核参数,1为偏置参数)\n",
    "    - 连接数: 卷积后每个特诊图有28x28个神经元,每个卷积核参数为(5x5+1)x6,所以该层连接数(5x5+1)x6x28x28=122304\n",
    "4. **S2层(池化层):6@14x14**\n",
    "    - 特诊图大小: 6x(28/2)x(28/2)=6x14x14\n",
    "    - 参数个数: 因为窗口为2x2,此时计算方式为 $sigmoid(w(\\sum_i^{n=4}(e_i)) + b)$;即将窗口4个元素相加乘以权值w,在加上b,最后做sigmoid激活,所以参数个数为6x(1+1)\n",
    "    - 连接数: (2x2+1)x6x14x14=5880\n",
    "5. **C3层(卷积层):6@10x10**\n",
    "    - 特诊图大小: 6x(14-5+1)x(14-5+1) = 6x10x10\n",
    "    - 参数个数: (5x5x3+1)x6 + (5x5x4+1)x9 + (5x5x6+1) = 1516 (详解见下)\n",
    "    - 连接数: 1516x10x10=151600\n",
    "6. **S4层(池化层):16@5x5**\n",
    "    - 特诊图大小: 16x(10/2)x(10/2) = 16x5x5\n",
    "    - 参数个数: 16x(1+1) = 16x2\n",
    "    - 连接数: 16x(2x2+1)x5x5\n",
    "7. **C5层(卷积层):120@5x5**\n",
    "    - 特诊图大小: 120x(5-5+1)x(5-5+1) = 120x1x1\n",
    "    - 参数个数: 120x(5x5x16+1) = 48120\n",
    "    - 连接数: 48120x(1x1)\n",
    "8. **F6层(全连接层):84**\n",
    "    - 特诊图大小: 84\n",
    "    - 参数个数: 84x(120+1) = 10164\n",
    "    - 连接数: 10164x(1x1)\n",
    "9. **OUTPUT层(输出层):10**\n",
    "    - 特诊图大小: 84; 使用径向基函数 $y_i = \\sum_{j=0}^{n=83}(x_j - w_{ij})^2$,其中,$x_i$是上一层输入$i$取0-9\n",
    "    - 参数个数: 84x10\n",
    "    - 连接数: 84x10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1.1 池化层计算参数\n",
    "\n",
    "> **池化层就两个参数,W和b,计算是将池化窗口的4个求和最后再通过sigmoid激活**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![池化层](img/5.5.1_lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1.2 LeNet C3层计算方式\n",
    "\n",
    "> 1. **此过程为S2(6个输入图) -> C3(16个输入图)**\n",
    "> 2. **C3第一个特征图是由S2中的[0, 1, 2]三个输入卷积得到,依次类推**\n",
    "> 3. **所以参数个数为 (5x5x3+1)x6 + (5x5x4+1)x9+ (5x5x6+1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![C3层](img/5.5.2_lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1.3 LeNet 输出示例图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![C3层](img/5.5.3_lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch.utils as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),  # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),  # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),  # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2)  # kernel_size, stride\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Sigmoid()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Sigmoid()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 没有指定device则使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式,会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义模型,\n",
    "                if 'is_training' in net.__code__.co_varnames: \n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()\n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('training on', device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "epoch 1, loss 1.8508, train acc 0.317, test acc 0.587, time 31.0 sec\n",
      "epoch 2, loss 0.9607, train acc 0.622, test acc 0.679, time 36.6 sec\n",
      "epoch 3, loss 0.7953, train acc 0.707, test acc 0.726, time 37.4 sec\n",
      "epoch 4, loss 0.7009, train acc 0.737, test acc 0.744, time 37.1 sec\n",
      "epoch 5, loss 0.6413, train acc 0.754, test acc 0.756, time 36.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr,  num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
