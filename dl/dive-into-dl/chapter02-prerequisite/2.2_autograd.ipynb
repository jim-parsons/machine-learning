{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.3.1+cpu'"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 梯度\n",
    "\n",
    "#### 2.3.1 Variable\n",
    "\n",
    "> 1. **`Variable` 是一个物理位置不变,里面内容不断变化的对象,而里面的内容就是tensor**\n",
    "> 2. **`Variable` 会构建一个计算图,`computaional graph`用于将所有计算步骤(节点)连接起来,最后误差反向传递,一次性将所有varibale里面的梯度计算出来**\n",
    "> 3. **通过`Variable.data` 输出tensor形式,通过`Variable.data.numpy()`输出numpy形式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "# requires_grad 是参不参与反向传播\n",
    "varibale = Variable(tensor, requires_grad=True)\n",
    "varibale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.5000, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_out = torch.mean(varibale ** 2)\n",
    "v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.5000, 3.0000],\n        [4.5000, 6.0000]])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟v_out误差反向传播\n",
    "# v_out = 1/4 * sum(variable ** 2)\n",
    "# 所以v_out的梯度就是, d(v_out)/d(varibale) = 1/4 * 2 * variable = varibale/2\n",
    "v_out.backward(retain_graph=True) # 等价于 v_out.backward(torch.tensor(1.))\n",
    "varibale.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3.2 梯度\n",
    "\n",
    "#### 反向传播梯度\n",
    "\n",
    "> 1. **在tensor里面,如果将属性requires_grad=True,则表示开始追踪其所有操作,完成后调用backend()完成所有梯度计算,最后将梯度`【累加】`到grad属性中**\n",
    "> 2. **调用Tensor.detach()方法将其从追踪记录中分离出来,这样防止被追踪,也可用with torch.no_grad() (多用于模型评估,因为此时不需要计算参数的梯度)**\n",
    "> 3. **`Function`是对变量操作的抽象, 和 `Tensor` 可构建一个记录整个计算过程的非循环图。Tensor.grad_fn属性记录对应的信息**\n",
    "\n",
    "#### 雅克比矩阵(Jacobian matrix)\n",
    "$$\n",
    "J = \n",
    "\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\cdots & \\frac{\\partial y_n}{\\partial x_n}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "如果 $v$ 是一个标量函数的 $l = g(\\vec y)$的梯度:\n",
    "\n",
    "$$ v = \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_1} & \\cdots & \\frac{\\partial l}{\\partial y_n} \\end{array}\\right)$$\n",
    "\n",
    "\n",
    "那么根据链式法则, $l$关于$\\vec x$的雅克比矩阵:\n",
    "\n",
    "\n",
    "$$ \n",
    "v \\cdot J = \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_1} & \\cdots & \\frac{\\partial l}{\\partial y_n} \\end{array}\\right) \n",
    "\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\cdots & \\frac{\\partial y_n}{\\partial x_n}\n",
    "\\end{array}\\right) \n",
    "= \\left(\\begin{array}{ccc} \\frac{\\partial l}{\\partial x_1} & \\cdots & \\frac{\\partial l}{\\partial x_n}  \\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "None\n"
    }
   ],
   "source": [
    "# x 为直接创建,所以没有grad_fn\n",
    "# 默认为flase,通过.requires_grad_(True)来设置\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "((<AccumulateGrad object at 0x0000023789A7CA20>, 0), (None, 0))\n"
    }
   ],
   "source": [
    "# 而此时y是通过计算得到,所以有对应的属性\n",
    "y = x + 2\n",
    "print(y.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nFalse\n"
    }
   ],
   "source": [
    "print(x.is_leaf)\n",
    "print(y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[2., 4.],\n        [6., 8.]], grad_fn=<ViewBackward>)\n"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([2.0000e+01, 2.0000e+00, 1.0000e-01, 1.0000e-02])\n"
    }
   ],
   "source": [
    "# z此时不是标量,因而求反向梯度时,需要传入一个同形的权重,如此时的v\n",
    "v = torch.tensor([[2.0, 0.2], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(1., grad_fn=<PowBackward0>)True\ntensor(1.)False\ntensor(2., grad_fn=<AddBackward0>)True\n"
    }
   ],
   "source": [
    "# 使用with torch.no_grad() 屏蔽梯度计算\n",
    "x = torch.tensor(1.0 ,requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    # 此时这个计算是没有算进梯度计算里面\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(2.)\n"
    }
   ],
   "source": [
    "$y_3 = y_1 + y_2 = x^2 + x^3 $ 当 $x = 1$时, $\\frac{d_y}{d_x} = 5 \\cdot x = 5$,但是由于 $ y_2 = x ^ 3 $ 这步计算被with no_grad()包裹,所以$y_2$的梯度不会回传,只有$y_1$的梯度才会"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1.])\nFalse\n"
    }
   ],
   "source": [
    "# 如果想要修改tensor的数值,但是又不希望被autograd记录,即不会影响反向传播,可以对tensor.data进行操作\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x.data)\n",
    "# x.data独立于计算图之外\n",
    "print(x.data.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([100.], requires_grad=True)\ntensor([2.])\n"
    }
   ],
   "source": [
    "y = 2 * x\n",
    "x.data *= 100 # 此处只修改值,不会记录在计算图内\n",
    "y.backward()\n",
    "print(x) # 修改data会影响到tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}