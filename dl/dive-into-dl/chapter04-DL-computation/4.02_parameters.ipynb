{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 模型参数的访问、初始化和共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "-0.5975306034088135\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 3), # pytorch默认初始化\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "\n",
    "print(net)\n",
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()\n",
    "print(Y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 访问模型参数\n",
    "\n",
    "> 1. **对于`Sequential`实例中含模型参数的层,可通过`Module`类的`parameters()`或者 `named_parameters()` 方法来访问所有参数(以迭代器形式)**\n",
    "> 2. **`named_parameters()` 返回的名字自动加上了索引作为前缀**\n",
    "> 3. **通过`net[index]`来访问网络任一层，此时返回的name没有索引前缀**\n",
    "> 4. **param的类型`torch.nn.parameter.Parameter`,是`Tensor`的子类,如果一个`Tensor`是`Parameter`,那么会自动被添加到模型参数列表**\n",
    "> 5. **`Parameter`是`Tensor`.可根据`data`访问参数值,`grad`访问梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        # weight1 是Parameter类型，会添加到网络参数中\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
    "        # 而weight2却不会\n",
    "        self.weight2 = torch.rand(20, 20)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "n = MyModel()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3011,  0.4198,  0.3230,  0.2946],\n",
      "        [ 0.2989,  0.3115, -0.3921,  0.2833],\n",
      "        [ 0.2363,  0.0392,  0.2317, -0.2515]])\n",
      "None\n",
      "tensor([[ 0.6614,  0.4827,  0.4717,  0.2036],\n",
      "        [-0.7584, -0.5535, -0.5410, -0.2335],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 自定义初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 7.0761e-05, -2.0239e-03, -1.0443e-02,  7.8113e-05],\n",
      "        [ 1.0639e-02,  4.1825e-03, -5.3804e-03, -5.0294e-03],\n",
      "        [ 5.2995e-03, -3.6706e-03, -2.2803e-02,  6.4969e-04]])\n",
      "2.weight tensor([[ 0.0117, -0.0019,  0.0093]])\n"
     ]
    }
   ],
   "source": [
    "# 使用init模块，初始化权重为N(0, 0.01)\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# 使用init模块，初始化偏置为 0\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param, val=0)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 自定义初始化方法\n",
    "\n",
    "> pytorch 源码\n",
    "```python\n",
    "def normal_(tensor, mean=0, std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重一半概率初始化为0，另一半概率初始化为[-15, -5]和[5, 10]的两个区间均匀分布\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        # 均匀分布\n",
    "        tensor.uniform_(-10, 10)\n",
    "        tensor *= (tensor.abs() >= 5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 7.8997,  0.0000,  7.5120, -9.1512],\n",
      "        [-7.8311, -0.0000,  8.1823, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000]])\n",
      "2.weight tensor([[-0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([2., 2., 2.])\n",
      "2.bias tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data += 1\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 共享模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 这两个线性层是一个对象\n",
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "# 模型参数里包含了梯度,所以反向传播是，这些共享参数的梯度是累加的\n",
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad) # 单次是3，所以两次是6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
